{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016ae1b2-bad1-4ab2-8de4-9c5cf7287488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_nav\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ppo.model import *\n",
    "from ppo.utils import init\n",
    "from evaluation import *\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c04d07-f024-4f6b-ac32-29b6b3949078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DelayedRNNPPO(NNBase):\n",
    "    '''\n",
    "    Quick and simple static RNN network with a FC followed by RNN followed by\n",
    "    2 layers of actor critic split\n",
    "    '''\n",
    "    def __init__(self, num_inputs, hidden_size=64,\n",
    "                auxiliary_heads=[]):\n",
    "        super(DelayedRNNPPO, self).__init__(True, hidden_size, hidden_size)\n",
    "        # parameters create self.GRU with hidden_size as recurrent_input_size and\n",
    "        #  hidden_size as recurrent_hidden_size\n",
    "        \n",
    "        self.auxiliary_heads = auxiliary_heads\n",
    "        self.has_auxiliary = True\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), np.sqrt(2))\n",
    "                \n",
    "        self.shared_layers = []\n",
    "        self.critic_layers = []\n",
    "        self.actor_layers = []\n",
    "        self.conv1d_layers = []\n",
    "        \n",
    "        # generate all the shared layers        \n",
    "        self.shared0 = nn.Sequential(init_(nn.Linear(num_inputs, hidden_size)),\n",
    "                                nn.Tanh())\n",
    "        self.critic0 = nn.Sequential(init_(nn.Linear(hidden_size, hidden_size)),\n",
    "                                nn.Tanh())\n",
    "        self.critic1 = nn.Sequential(init_(nn.Linear(hidden_size, hidden_size)),\n",
    "                                nn.Tanh())\n",
    "        self.actor0 = nn.Sequential(init_(nn.Linear(hidden_size, hidden_size)),\n",
    "                                nn.Tanh())\n",
    "        self.actor1 = nn.Sequential(init_(nn.Linear(hidden_size, hidden_size)),\n",
    "                                nn.Tanh())        \n",
    "        self.critic_head = init_(nn.Linear(hidden_size, 1))\n",
    "        \n",
    "            \n",
    "        self.auxiliary_layers = []\n",
    "        self.auxiliary_output_idxs = [] # indexes for generating auxiliary outputs\n",
    "        self.auxiliary_layer_types = [] # 0 linear, 1 distribution\n",
    "        self.auxiliary_output_sizes = []\n",
    "        # generate auxiliary outputs\n",
    "        current_auxiliary_output_idx = 0\n",
    "\n",
    "        self.has_auxiliary = False\n",
    "        self.train()\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, rnn_hxs, masks, deterministic=False, with_activations=False):\n",
    "        \"\"\"Same as forward function but this will pass back all intermediate values\n",
    "\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        auxiliary_preds = [None for i in range(len(self.auxiliary_output_sizes))]\n",
    "        x = inputs\n",
    "\n",
    "        shared_activations = []\n",
    "        actor_activations = []\n",
    "        critic_activations = []\n",
    "\n",
    "        x = self.shared0(x)\n",
    "        x = x.unsqueeze(0)\n",
    "        shared_activations.append(x)\n",
    "        x, rnn_hxs = self._forward_gru(x, rnn_hxs, masks)\n",
    "        shared_activations.append(x)\n",
    "        \n",
    "        actor_x = self.actor0(x)\n",
    "        actor_activations.append(actor_x)\n",
    "        actor_x = self.actor1(actor_x)\n",
    "        actor_activations.append(actor_x)\n",
    "        \n",
    "        critic_x = self.critic0(x)\n",
    "        critic_activations.append(critic_x)\n",
    "        critic_x = self.critic1(x)\n",
    "        critic_activations.append(critic_x)\n",
    "                    \n",
    "        # Finally get critic value estimation\n",
    "        critic_val = self.critic_head(critic_x)\n",
    "\n",
    "        outputs = {\n",
    "            'value': critic_val,\n",
    "            'actor_features': actor_x,\n",
    "            'rnn_hxs': rnn_hxs,\n",
    "        }\n",
    "        \n",
    "        if self.has_auxiliary:\n",
    "            outputs['auxiliary_preds'] = auxiliary_preds\n",
    "        if with_activations:\n",
    "            outputs['activations'] = {\n",
    "                'shared_activations': shared_activations,\n",
    "                'actor_activations': actor_activations,\n",
    "                'critic_activations': critic_activations\n",
    "            }        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0185cef2-dbf7-468f-96a9-87c66caf03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('NavEnv-v0')\n",
    "envs = simple_vec_envs()\n",
    "env = envs.envs[0]\n",
    "actor_critic = Policy(env.observation_space.shape, env.action_space, base='DelayedRNNPPO')\n",
    "base = actor_critic.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ab53a7-eedf-4ab1-a437-ad4927ffa113",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hxs = torch.zeros(\n",
    "    1, actor_critic.recurrent_hidden_state_size)\n",
    "masks = torch.zeros(1, 1)\n",
    "obs = torch.Tensor(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04a44f1-989c-4beb-9cf9-98baad3ee892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output = actor_critic.act(obs, rnn_hxs, masks)\n",
    "# output = base(obs, rnn_hxs, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fad54f2-79e2-44a3-b877-a5d996f1a2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': tensor([[0.2708]], grad_fn=<AddmmBackward0>),\n",
       " 'actor_features': tensor([[-0.1533,  0.1356, -0.2292,  0.0867, -0.0468, -0.0857,  0.1130,  0.2400,\n",
       "           0.0661, -0.2064,  0.0300, -0.1709, -0.0264, -0.1074,  0.1676, -0.0953,\n",
       "           0.0723,  0.0176, -0.1280,  0.0971,  0.1476, -0.0412, -0.2048,  0.0342,\n",
       "           0.0953, -0.1104, -0.2634, -0.0140, -0.0317,  0.2504, -0.1643,  0.3934,\n",
       "          -0.1132, -0.0138,  0.0387,  0.2093,  0.1442, -0.0193, -0.2719, -0.0531,\n",
       "           0.0040, -0.0480,  0.3208, -0.0987,  0.1015, -0.0185, -0.2250, -0.1349,\n",
       "           0.0475,  0.0249,  0.0211, -0.1089,  0.2073, -0.0637, -0.0624,  0.0318,\n",
       "           0.1073,  0.1470, -0.2576,  0.0703, -0.0240, -0.1081, -0.1692, -0.2053]],\n",
       "        grad_fn=<TanhBackward0>),\n",
       " 'rnn_hxs': tensor([[-4.5651e-02,  3.0627e-02, -3.9439e-02, -1.4857e-01,  9.6118e-02,\n",
       "          -1.7480e-01,  2.7043e-02, -4.2615e-02,  1.5409e-02, -1.8125e-02,\n",
       "          -1.2344e-02, -9.6874e-02,  1.9311e-01,  2.9294e-02,  9.0790e-02,\n",
       "          -5.4984e-02, -5.5998e-02,  1.1730e-01, -1.2288e-01,  5.0593e-02,\n",
       "          -4.9734e-02,  9.1791e-02,  1.2950e-01,  3.0131e-02,  3.4579e-03,\n",
       "           5.1058e-02,  1.3914e-02, -4.0661e-02, -5.8854e-02,  4.2670e-02,\n",
       "           6.0479e-04, -3.4521e-02,  1.4993e-01,  1.4445e-02, -4.5632e-02,\n",
       "          -7.5186e-02,  1.4125e-02,  4.9599e-02, -5.2422e-02,  5.2293e-02,\n",
       "           9.8095e-03,  1.0045e-02, -2.9084e-02,  6.6756e-02, -5.4245e-02,\n",
       "           1.0728e-01, -8.9816e-02,  1.0722e-01,  1.7610e-02,  8.8490e-03,\n",
       "           9.6034e-02,  9.1645e-02, -1.1978e-01,  1.0386e-02, -1.3265e-04,\n",
       "           5.2259e-02,  1.0813e-01,  9.6926e-02, -2.4908e-02, -3.3243e-04,\n",
       "           8.8311e-02, -6.3490e-02,  8.0739e-02, -6.7406e-02]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " 'auxiliary_preds': None,\n",
       " 'action': tensor([[2]]),\n",
       " 'action_log_probs': tensor([[-1.3862]], grad_fn=<UnsqueezeBackward0>)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd3e4013-e049-41ee-933e-9db312c217e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixedCategorical(logits: torch.Size([1, 4]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_critic.dist(output['actor_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87b0c001-40d0-44e0-8537-5220dd98ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = actor_critic.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28613c21-afe4-4fe4-993c-400ffa12fde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3107, -0.2540,  0.1618, -0.1383,  0.0137,  0.0759,  0.1995,  0.0686,\n",
       "          0.2912,  0.2471, -0.0391, -0.1496,  0.1112,  0.2538,  0.3188,  0.1102,\n",
       "          0.5746, -0.0592,  0.4906,  0.0756, -0.4205,  0.2638, -0.1815, -0.4008,\n",
       "         -0.1849,  0.3262, -0.0193, -0.0456, -0.1655, -0.0774, -0.5269,  0.2804,\n",
       "          0.1704, -0.1933, -0.2009, -0.2227, -0.2322,  0.1751, -0.0233, -0.2919,\n",
       "         -0.4070,  0.0553,  0.0229, -0.0602,  0.0235, -0.0424, -0.1237,  0.0351,\n",
       "          0.2804, -0.5164,  0.1617, -0.3315,  0.0872,  0.3726, -0.2472,  0.1194,\n",
       "         -0.0350,  0.2464, -0.4908,  0.4867,  0.5268,  0.1418, -0.2745, -0.2767]],\n",
       "       grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.shared0(obs).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea2ce7e7-a5f9-4899-8065-0e364781eaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb224d99-86a6-4ce8-85c9-163bfedaa240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('NavEnv-v0')\n",
    "envs = simple_vec_envs()\n",
    "actor_critic = Policy(env.observation_space.shape, env.action_space, base='FlexBaseAux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "784396a1-d9e7-495c-8135-8963b03ff93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb3fd7eb-fcb8-4b3e-a1c2-d55dce2f767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hxs = torch.zeros(\n",
    "    1, actor_critic.recurrent_hidden_state_size)\n",
    "masks = torch.zeros(1, 1)\n",
    "obs = torch.Tensor(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c64a4a76-7ddf-4632-8b86-405afa053a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = actor_critic.act(torch.Tensor(obs), rnn_hxs, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0fef3a-9b88-498e-bd49-e1efd3959524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradients import *\n",
    "\n",
    "\n",
    "\n",
    "def initialize_ppo_training(model=None, obs_rms=None, env_name='NavEnv-v0', env_kwargs={}, make_env=True,\n",
    "                            agent_base='LoudPPO', nn_base_kwargs={}, recurrent=True,\n",
    "                            num_steps=10, num_processes=1, seed=0, ppo_epoch=4, clip_param=0.5,\n",
    "                            num_mini_batch=1, value_loss_coef=0.5, entropy_coef=0.01, \n",
    "                            auxiliary_loss_coef=0.3, gamma=0.99, lr=7e-4, eps=1e-5, max_grad_norm=0.5,\n",
    "                            log_dir='/tmp/gym/', device=torch.device('cpu'), \n",
    "                            capture_video=False, take_optimizer_step=True,\n",
    "                            normalize=True, obs=None, aux_wrapper_kwargs={},\n",
    "                            auxiliary_truth_sizes=[]):\n",
    "    \"\"\"Generate training objects, specifically setting up everything to generate gradients\n",
    "        Important parameters:\n",
    "            model, obs_rms, env_kwargs, num_steps (batch_size), num_processes, seed, \n",
    "            ppo_epoch (usually set=1), take_optimizer_step (usually set=False)\n",
    "\n",
    "    Args:\n",
    "        model (Policy, optional): Policy object (e.g. from load_model_and_env). If not provided\n",
    "            generate a fresh model with nn_base and nn_base_kwargs\n",
    "        obs_rms (RunningMeanStd, optional): obs_rms object for vectorized envs. Defaults to None.\n",
    "        env_name (str, optional): Defaults to 'NavEnv-v0'.\n",
    "        env_kwargs (dict, optional): Defaults to {}.\n",
    "        nn_base (str, optional): Used to create model if model is not provided. \n",
    "            Defaults to 'FlexBase'.\n",
    "        agent_base (str, optional): Used to create trainer object. Defaults to 'LoudPPO',\n",
    "            can also use 'PPO' and 'DecomposeGradPPO'.\n",
    "        nn_base_kwargs (dict, optional): Used to create model if model is not provided. \n",
    "            Defaults to {}.\n",
    "        recurrent (bool, optional): Used if model==None. Defaults to True.\n",
    "        num_steps (int, optional): Batch size to use. Defaults to 10.\n",
    "        num_processes (int, optional): Number of concurrent processes. Defaults to 1.\n",
    "        seed (int, optional): Randomizer seed. Defaults to 0.\n",
    "        ppo_epoch (int, optional): Number of epochs to run for PPO. Defaults to 4. Usually\n",
    "            we will want to set this to 1 to collect grads with\n",
    "        clip_param (float, optional): PPO clip param. Defaults to 0.5.\n",
    "        num_mini_batch (int, optional): Number of minibatches to split training rollouts into. \n",
    "            Defaults to 1.\n",
    "        value_loss_coef (float, optional): Value loss weighting. Defaults to 0.5.\n",
    "        entropy_coef (float, optional): Entropy loss weighting. Defaults to 0.01.\n",
    "        auxiliary_loss_coef (float, optional): Auxiliary loss weighting. Defaults to 0.3.\n",
    "        gamma (float, optional): Discount factor. Defaults to 0.99.\n",
    "        lr (_type_, optional): Learning rate. Defaults to 7e-4.\n",
    "        eps (_type_, optional): _description_. Defaults to 1e-5.\n",
    "        max_grad_norm (float, optional): Cap on gradient steps. Defaults to 0.5.\n",
    "        log_dir (str, optional): Logging directory. Defaults to '/tmp/gym/'.\n",
    "        device (_type_, optional): Device to run on. Defaults to torch.device('cpu').\n",
    "        capture_video (bool, optional): Whether to capture video on episodes. Defaults to False.\n",
    "        take_optimizer_step (bool, optional): Whether to actually take gradient update\n",
    "            step. Defaults to True.\n",
    "        normalize (bool, optional): Whether to normalize vectorized environment observations. \n",
    "            Defaults to True.\n",
    "        obs (torch.Tensor, optional): Need to pass the first observation if not making new environments\n",
    "\n",
    "    Returns:\n",
    "        agent, envs, rollouts\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize vectorized environments\n",
    "    # envs = make_vec_envs(env_name, seed, num_processes, gamma, log_dir, device, False,\n",
    "    #                      capture_video=capture_video, env_kwargs=env_kwargs)\n",
    "    if make_env:\n",
    "        envs = make_vec_envs(env_name, seed, num_processes, gamma, log_dir, device, False,\n",
    "                            capture_video=capture_video, env_kwargs=env_kwargs, normalize=normalize,\n",
    "                            **aux_wrapper_kwargs)\n",
    "    else:\n",
    "        envs = None\n",
    "\n",
    "    env = gym.make(env_name, **env_kwargs)\n",
    "\n",
    "    if model is None:\n",
    "        nn_base = 'DelayedRNNPPO'\n",
    "        model = Policy(env.observation_space.shape,\n",
    "                       env.action_space,\n",
    "                       base=nn_base,\n",
    "                       base_kwargs={'recurrent': recurrent,\n",
    "                           **nn_base_kwargs})\n",
    "        model.to(device)\n",
    "    \n",
    "    #Wrap model with an agent algorithm object\n",
    "    # agent = algo.PPO(model, clip_param, ppo_epoch, num_mini_batch,\n",
    "    try:\n",
    "        # if new_aux:\n",
    "        #     agent = PPOAux(model, clip_param, ppo_epoch, num_mini_batch,\n",
    "        #             value_loss_coef, entropy_coef, auxiliary_loss_coef, lr=lr,\n",
    "        #             eps=eps, max_grad_norm=max_grad_norm)\n",
    "        # else:\n",
    "        base = globals()[agent_base]\n",
    "        agent = base(model, clip_param, ppo_epoch, num_mini_batch,\n",
    "                        value_loss_coef, entropy_coef, auxiliary_loss_coef, lr=lr,\n",
    "                        eps=eps, max_grad_norm=max_grad_norm,\n",
    "                        take_optimizer_step=take_optimizer_step)\n",
    "    except:\n",
    "        print('Model type not found')\n",
    "        return False\n",
    "\n",
    "\n",
    "    #Initialize storage\n",
    "    rollouts = RolloutStorageAux(num_steps, num_processes, env.observation_space.shape, env.action_space,\n",
    "                        model.recurrent_hidden_state_size, model.auxiliary_output_sizes,\n",
    "                        auxiliary_truth_sizes)\n",
    "    #Storage objects initializes a bunch of empty tensors to store information, e.g.\n",
    "    #obs has shape (num_steps+1, num_processes, obs_shape)\n",
    "    #rewards has shape (num_steps, num_processes, 1)\n",
    "    \n",
    "\n",
    "    #If loading a previously trained model, pass an obs_rms object to set the vec envs to use\n",
    "    \n",
    "    if normalize and obs_rms != None:\n",
    "        vec_norm = utils.get_vec_normalize(envs)\n",
    "        if vec_norm is not None and obs_rms is not None:\n",
    "            vec_norm.obs_rms = obs_rms\n",
    "\n",
    "        \n",
    "    #obs, recurrent_hidden_states, value_preds, returns all have batch size num_steps+1\n",
    "    #rewards, action_log_probs, actions, masks, auxiliary_preds, auxiliary_truths all have batch size num_steps\n",
    "    if make_env:\n",
    "        obs = envs.reset()\n",
    "    elif obs == None:\n",
    "        raise Exception('No obs passed and no env created, storage cannot be initialized')\n",
    "    rollouts.obs[0].copy_(obs)\n",
    "    rollouts.to(device)\n",
    "    \n",
    "    return agent, envs, rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f902a15-00ae-453b-82c9-281a46acb63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, envs, rollouts = initialize_ppo_training(env_kwargs={'rew_structure': 'goal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4efc90c7-6bd4-4e62-a744-6b74e0c77f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({},\n",
       " [[{'auxiliary': array([], dtype=float64)}],\n",
       "  [{'auxiliary': array([], dtype=float64)}],\n",
       "  [{'auxiliary': array([], dtype=float64)}],\n",
       "  [{'auxiliary': array([], dtype=float64)}],\n",
       "  [{'auxiliary': array([], dtype=float64)}],\n",
       "  [{'auxiliary': array([], dtype=float64)}],\n",
       "  [{'auxiliary': array([], dtype=float64)}],\n",
       "  [{'auxiliary': array([], dtype=float64)}],\n",
       "  [{'auxiliary': array([], dtype=float64)}],\n",
       "  [{'auxiliary': array([], dtype=float64)}]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "populate_rollouts(agent.actor_critic, envs, rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a4252d-aabe-4e5a-bb4b-a7b5e0c168b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.optimizer.zero_grad()\n",
    "next_value = agent.actor_critic.get_value(rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "                             rollouts.masks[-1]).detach()\n",
    "rollouts.compute_returns(next_value, False, 0.99, 0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7826961a-10f8-493e-9008-946277eb52c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m value_loss, action_loss, dist_entropy, approx_kl, clipfracs, auxiliary_loss, \\\n\u001b[1;32m----> 2\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollouts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Work\\github\\ppo_aux\\write_and_test\\gradients.py:1027\u001b[0m, in \u001b[0;36mLoudPPO.update\u001b[1;34m(self, rollouts)\u001b[0m\n\u001b[0;32m   1022\u001b[0m obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n\u001b[0;32m   1023\u001b[0m    value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, \\\n\u001b[0;32m   1024\u001b[0m     adv_targ, auxiliary_pred_batch, auxiliary_truth_batch \u001b[38;5;241m=\u001b[39m sample\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# Reshape to do in a single forward pass for all steps\u001b[39;00m\n\u001b[1;32m-> 1027\u001b[0m values, action_log_probs, dist_entropy, _, auxiliary_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurrent_hidden_states_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m logratio \u001b[38;5;241m=\u001b[39m action_log_probs \u001b[38;5;241m-\u001b[39m old_action_log_probs_batch \n\u001b[0;32m   1032\u001b[0m ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(logratio)\n",
      "File \u001b[1;32m~\\Desktop\\Work\\github\\ppo_aux\\write_and_test\\..\\ppo\\model.py:129\u001b[0m, in \u001b[0;36mPolicy.evaluate_actions\u001b[1;34m(self, inputs, rnn_hxs, masks, action)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     value, actor_features, rnn_hxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase(inputs, rnn_hxs, masks)\n\u001b[1;32m--> 129\u001b[0m     auxiliary \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m    131\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist(actor_features)\n\u001b[0;32m    133\u001b[0m action_log_probs \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_probs(action)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "value_loss, action_loss, dist_entropy, approx_kl, clipfracs, auxiliary_loss, \\\n",
    "    grads = agent.update(rollouts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c9ce367-fd7a-4ddd-857b-3ec000bebc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor_critic.is_recurrent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
